{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='data/banner.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [Using large language models for structured information extraction from the Innocence Project New Orleans' wrongful conviction case files](#using-large-language-models-for-structured-information-extraction-from-the-ipno-case-files)\n",
    "  - [Introduction](#introduction)\n",
    "  - [Structured data extraction with regular expressions](#structured-data-extraction-with-regex)\n",
    "  - [Structured data extraction with a large language model](#structured-data-extraction-with-an-llm)\n",
    "- [Evaluations, issues, improvements](#evaluations-issues-improvements)\n",
    "- [Future research/next steps](#future-researchnext-steps)\n",
    "  - [Fine-tuning](#fine-tuning)\n",
    "- [Appendix](#appendix)\n",
    "  - [Testing with GPT-4](#gpt-4-evaluation)\n",
    "  - [Acknowledgements](#acknowledgements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"using-large-language-models-for-structured-information-extraction-from-the-ipno-case-files\"></a>\n",
    "# Using large language models for structured information extraction from the Innocence Project New Orleans' wrongful conviction case files\n",
    "\n",
    "- [Ayyub Ibrahim](https://ip-no.org/who-we-are/staff/#ai), Director of Research for LLEAD, Innocence Project of New Orleans\n",
    "- [Tarak Shah](https://hrdag.org/people/tarak-shah/), Data Scientist, Human Rights Data Analysis Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exoneration documents, documents acquired during the legal processes that seek to rectify miscarriages of justice, offer invaluable insights into wrongful conviction cases. Particularly, they illuminate the roles and actions of law enforcement personnel. Yet, the sheer volume and lack of structure in these documents pose challenges for researchers, lawyers, and advocates dedicated to transparency and justice. In 2022, the Innocence Project New Orleans (IPNO) launched the Louisiana [Law Enforcement Accountability Database (LLEAD)](https://llead.co/), a consolidation of data from over 500 law enforcement agencies in Louisiana. To date, LLEAD hosts details of over 40,000 allegations of misconduct spanning 194 agencies across 48 of Louisiana's 64 parishes. This initiative is the first state-wide database of its kind. LLEAD is already an essential tool for exoneration work, and including wrongful conviction information in the database would make it even more useful. For example in Orleans Parish, Louisiana, 78% of wrongful convictions have been linked to law enforcement's failure to share exculpatory evidence with the defense, a rate more than double the national average. Given this backdrop, we seek to make these collections searchable and useful for lawyers, advocates, and community members to better investigate patterns of police misconduct and corruption. In order to do so, we rely on a multi-stage process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Metadata Compilation: We started by compiling a comprehensive CSV index. This structured approach forms the foundation of our file management system, enabling file retrieval and basic deduplication. The metadata we organize in this step includes:\n",
    "\n",
    "    - file path and name\n",
    "    - file type\n",
    "    - sha1 content hash: we truncate this to create unique file identifiers\n",
    "    - file size and number of pages\n",
    "    - case ID: when we scanned the documents, we organized them into directories organized by case ID, here we pluck and validate the directory name to add a structured case id field to our metadata index.\n",
    "\n",
    "2) Page classification: The documents in the collection are varied, representing all documents produced or acquired in the course of an exoneration case, with case timelines going back decades. After some internal review and discussions with the Innocence Project New Orleans (IPNO) case management team, we narrowed our focus to three types of documents:\n",
    "    - police reports: include mentions of officers involved in the arrest that led to the wrongful conviction, or related arrests\n",
    "    - transcripts: court transcripts, recorded by clerks of the court, where officers appear as witnesses and describe their role in the conviction (including making the arrest, transporting evidence, interrogating the accused).\n",
    "    - testimonies: witness testimony, which will include testimony from officers involved in the conviction\n",
    "    \n",
    "    Page classification involves building a classification model to categorize files (or page sequences within files) into these different types of documents. One approach is to fine-tune a pretrained convolutional neural network to label thumbnail images of document pages. Using thumbnails is advantageous because they are smaller files, resulting in faster processing and reduced computational resource consumption. This makes them an effective approach for retrieving specific types of documents from disorganized collections, as described in [*Evaluation of Deep Convolutional Nets for Document Image Classification and Retrieval*](https://arxiv.org/abs/1502.07058). In order to use this technique, we needed training data and a pretrained model. To quickly assemble a training data set for our page classifier, we started by noticing that in many cases the file name indicated the document type. These documents were scanned by many people at different times, so we could not rely on this heuristic for comprehensive categorization of documents, but there was more than enough there to jumpstart our training process. We collected our initial training data by probing filenames for specific search terms, and reviewing and confirming that we had inferred the correct document types from the filenames. Once we had training data, we used [FastAI](https://docs.fast.ai/) to fine-tune the `ResNet34` architecture, pretrained on [ImageNet](https://www.image-net.org/), to identify reports, transcripts, and testimonies based on page thumbnails. With the trained classifier, we were able to measure generalization performance on documents that couldn't be classified via filename, and we were also better able to target additional training data, for example by reviewing pages where the classifier had low confidence about its prediction. \n",
    "\n",
    "3) Information Extraction: Currently, we're engaged in extracting structured information from the documents we've identified, and that work is the focus of the current post. Our goal is to extract structured information related to each police officer or prosecutor mentioned in the documents, such as their names, ranks, and roles in the wrongful conviction.\n",
    "\n",
    "4) Deduplication: The previous step leaves us with many distinct mentions, but some individuals are mentioned many times, within the same case or across cases. Here we rely on HRDAG's [extensive experience with database deduplication](https://hrdag.org/tech-notes/adaptive-blocking-writeup-1.html) to create a unique index of officers and prosecutors involved in wrongful convictions, and a record and the role or roles they had in the wrongful conviction.\n",
    "\n",
    "5) Cross-referencing: In the final stage, we'll cross-reference the officer names and roles we've extracted with the Louisiana Law Enforcement Accountability Database [(LLEAD.co)](https://llead.co/). This stage will assist us in identifying other individuals linked with the implicated officers, such as their partners, those co-accused in misconduct complaints, or those co-involved in use-of-force incidents. The list of officers associated with previous wrongful conviction cases can then be cross-referenced with the IPNO's internal data on potential wrongful convictions with the aim of uncovering new instances of wrongful convictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: Rows, Index: Columns\n",
      "(308770, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>filename</th>\n",
       "      <th>filehash</th>\n",
       "      <th>filesize</th>\n",
       "      <th>uid</th>\n",
       "      <th>filetype</th>\n",
       "      <th>case_id</th>\n",
       "      <th>png</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../input/wrongful-convictions-docs/BRIGHT_Dan_...</td>\n",
       "      <td>Bright, Dan compensation check August 2013.pdf</td>\n",
       "      <td>2c1ba05aacf163c83319401f905a20287ddad5e6</td>\n",
       "      <td>182755</td>\n",
       "      <td>2c1ba05a</td>\n",
       "      <td>pdf</td>\n",
       "      <td>BRIGHT_Dan_302392</td>\n",
       "      <td>output/2c/1b/2c1ba05a_p0001.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../input/wrongful-convictions-docs/BRIGHT_Dan_...</td>\n",
       "      <td>Bright, Dan supplemental compensation appln (f...</td>\n",
       "      <td>f51f6b34d66366770b0993f297f2dcc06d161b5f</td>\n",
       "      <td>500097</td>\n",
       "      <td>f51f6b34</td>\n",
       "      <td>pdf</td>\n",
       "      <td>BRIGHT_Dan_302392</td>\n",
       "      <td>output/f5/1f/f51f6b34_p0001.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../input/wrongful-convictions-docs/BRIGHT_Dan_...</td>\n",
       "      <td>Bright, Dan supplemental compensation appln (f...</td>\n",
       "      <td>f51f6b34d66366770b0993f297f2dcc06d161b5f</td>\n",
       "      <td>500097</td>\n",
       "      <td>f51f6b34</td>\n",
       "      <td>pdf</td>\n",
       "      <td>BRIGHT_Dan_302392</td>\n",
       "      <td>output/f5/1f/f51f6b34_p0002.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../input/wrongful-convictions-docs/BRIGHT_Dan_...</td>\n",
       "      <td>Bright, Dan supplemental compensation appln (f...</td>\n",
       "      <td>f51f6b34d66366770b0993f297f2dcc06d161b5f</td>\n",
       "      <td>500097</td>\n",
       "      <td>f51f6b34</td>\n",
       "      <td>pdf</td>\n",
       "      <td>BRIGHT_Dan_302392</td>\n",
       "      <td>output/f5/1f/f51f6b34_p0003.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../input/wrongful-convictions-docs/BRIGHT_Dan_...</td>\n",
       "      <td>Bright, Dan supplemental compensation appln (f...</td>\n",
       "      <td>f51f6b34d66366770b0993f297f2dcc06d161b5f</td>\n",
       "      <td>500097</td>\n",
       "      <td>f51f6b34</td>\n",
       "      <td>pdf</td>\n",
       "      <td>BRIGHT_Dan_302392</td>\n",
       "      <td>output/f5/1f/f51f6b34_p0004.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filepath  \\\n",
       "0  ../input/wrongful-convictions-docs/BRIGHT_Dan_...   \n",
       "1  ../input/wrongful-convictions-docs/BRIGHT_Dan_...   \n",
       "2  ../input/wrongful-convictions-docs/BRIGHT_Dan_...   \n",
       "3  ../input/wrongful-convictions-docs/BRIGHT_Dan_...   \n",
       "4  ../input/wrongful-convictions-docs/BRIGHT_Dan_...   \n",
       "\n",
       "                                            filename  \\\n",
       "0     Bright, Dan compensation check August 2013.pdf   \n",
       "1  Bright, Dan supplemental compensation appln (f...   \n",
       "2  Bright, Dan supplemental compensation appln (f...   \n",
       "3  Bright, Dan supplemental compensation appln (f...   \n",
       "4  Bright, Dan supplemental compensation appln (f...   \n",
       "\n",
       "                                   filehash  filesize       uid filetype  \\\n",
       "0  2c1ba05aacf163c83319401f905a20287ddad5e6    182755  2c1ba05a      pdf   \n",
       "1  f51f6b34d66366770b0993f297f2dcc06d161b5f    500097  f51f6b34      pdf   \n",
       "2  f51f6b34d66366770b0993f297f2dcc06d161b5f    500097  f51f6b34      pdf   \n",
       "3  f51f6b34d66366770b0993f297f2dcc06d161b5f    500097  f51f6b34      pdf   \n",
       "4  f51f6b34d66366770b0993f297f2dcc06d161b5f    500097  f51f6b34      pdf   \n",
       "\n",
       "             case_id                              png  \n",
       "0  BRIGHT_Dan_302392  output/2c/1b/2c1ba05a_p0001.png  \n",
       "1  BRIGHT_Dan_302392  output/f5/1f/f51f6b34_p0001.png  \n",
       "2  BRIGHT_Dan_302392  output/f5/1f/f51f6b34_p0002.png  \n",
       "3  BRIGHT_Dan_302392  output/f5/1f/f51f6b34_p0003.png  \n",
       "4  BRIGHT_Dan_302392  output/f5/1f/f51f6b34_p0004.png  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def read_index():\n",
    "    summary = pd.read_csv(\"data/index.csv\")\n",
    "    print(\"Index: Rows, Index: Columns\")\n",
    "    print(summary.shape)\n",
    "    summary = summary.head(5)\n",
    "    return summary\n",
    "\n",
    "read_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"structured-data-extraction-with-regex\"></a>\n",
    "# Structured Data Extraction with Regex\n",
    "\n",
    "A primary task in our process is extracting officer information from documents – specifically, the officer's name and the role the officer played in the wrongful conviction. The extraction of such information is crucial for understanding the dynamics and potential lapses that led to the conviction. Given the importance of this task, it's essential to approach it with a methodology that ensures accuracy and comprehensiveness.\n",
    "\n",
    "We initially considered a regex-based solution for this extraction task. Regular expressions, or regexes, are powerful tools for pattern matching within text. However, as we delved deeper into our data, we realized that the complexity and variability of the content rendered regex less than ideal. While regex excels at identifying specific patterns within text, it often struggles with variations in language and nuances that commonly appear in natural language texts, such as police reports and court transcripts.\n",
    "\n",
    "Consider the text string from a court transcript reading, \"Sergeant Ruiz was mentioned as being involved in the joint investigation with Detective Martin Venezia regarding the Seafood City burglary and the murder of Kathy Ulfers.\" Such a sentence poses challenges for regex due to its inability to capture semantic context. Without understanding the broader narrative, regex cannot infer that Sergeant Ruiz acted as a lead detective in Kathy Ulfers' murder case.\n",
    "\n",
    "To further highlight the limitations of regex in handling such tasks, we designed a simple baseline model. Instead of attempting to capture the full scope of officer information extraction, this model focuses solely on extracting officer names as a starting point. This choice was intentional; by narrowing down the task, we hoped to provide a clear example of the strengths and weaknesses of regex in the context of real-world data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r\"(detective|sergeant|lieutenant|captain|corporal|deputy|criminalist|technician|investigator\"\n",
    "                     r\"|det\\.|sgt\\.|lt\\.|cpt\\.|cpl\\.|dty\\.|tech\\.|dr\\.)\\s+([A-Z][A-Za-z]*(\\s[A-Z][A-Za-z]*)?)\", re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing our baseline model, we tested its performance on two different sets of data: police reports and court transcripts.\n",
    "\n",
    "**Police Reports Results:**\n",
    "- **Precision**: 0.845\n",
    "This indicates that among the instances our model predicted as officer names, 84.5% of them were indeed officer names. A high precision suggests that the model is quite reliable in its positive predictions.\n",
    "- **Recall**: 0.518\n",
    "This metric reveals that our model was able to identify only 51.8% of the actual officer names present in the police reports. A lower recall signifies that while our predictions are accurate, we are missing out on a significant number of true officer names.\n",
    "- **F1 Score**: 0.614\n",
    "The F1 score harmonizes precision and recall, giving us a balanced view of the model's performance. At 0.614, it suggests that there is room for improvement, especially in capturing more true positives without sacrificing precision.\n",
    "- **F_beta**: 0.549\n",
    "This score is another harmonic mean of precision and recall but gives more weight to recall. A score of 0.549 further emphasizes the model's challenges in identifying all true positives.\n",
    "\n",
    "**Court Transcripts Results:**\n",
    "- **Precision**: 0.8656\n",
    "Similar to the police reports, our model displayed high precision on court transcripts, indicating its reliability in positive predictions.\n",
    "- **Recall**: 0.4281\n",
    "However, the recall is notably lower on the court transcripts, meaning our model missed out on more than half of the actual officer names present in these documents.\n",
    "- **F1 Score**: 0.5461\n",
    "The F1 score for court transcripts is lower than that for police reports, suggesting a more pronounced trade-off between precision and recall in this dataset.\n",
    "- **F_beta**: 0.4663\n",
    "Once again, the F_beta score underscores the need for improving recall without compromising precision.\n",
    "\n",
    "While our regex-based baseline model exhibits high precision on both datasets, it struggles notably with recall. This indicates that while the names it identifies as officers are likely correct, it misses a substantial number of actual officer names present in the documents. These findings further emphasize the challenges of using regex alone for such a complex task and underscore the need for more advanced techniques that can capture the nuances and variations in language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"structured-data-extraction-with-an-llm\"></a>\n",
    "# Structured Data Extraction with an LLM\n",
    "\n",
    "An alternative approach is to prompt a generative language model with the document text along with a query describing our required output. One challenge with this approach is that the documents we're processing may be hundreds of pages long, whereas generative models have a limit to the length of the prompt you supply. We needed a way to pull out of each document just the chunks of text where the relevant officer information appears, to provide a more helpful prompt.\n",
    "\n",
    "We split up the problem into two steps, identifying the relevant chunks of text content, and then extracting structured officer information from those chunks. We use [Langchain](https://docs.langchain.com/docs/), a natural language processing library, to manage this pipeline, and use OpenAI's language model, GPT-3-Turbo-16k as the language model powering the pipeline. \n",
    "\n",
    "For the first step, identifying the relevant chunks of text within the larger document, we used the approach outlined in [Precise Zero-Shot Dense Retrieval without Relevance Labels](https://arxiv.org/abs/2212.10496). This approach splits our information retrieval task into multiple steps:\n",
    "\n",
    "1. We feed our query asking for names and roles of mentioned officers to an instruction-following generative language model, instructing it to compose a \"hypothetical\" document in response to the query.\n",
    "2. We embed the hypothetical document\n",
    "3. We chunk the document text into overlapping chunks, and calculate embeddings for each chunk using the same embedding system we used for the hypothetical document\n",
    "4. We use [Faiss](https://faiss.ai/) to do similarity search, using similarity to our hypothetical document embeddings to identify chunks of relevant text content.\n",
    "\n",
    "Here is the method we use to generate hypothetical embeddings. The resulting object can be used to embed chunks of text, enabling efficient similarity search over them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE_HYDE = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"\n",
    "    You're an AI assistant specializing in criminal justice research. \n",
    "    Your main focus is on identifying the names and providing detailed context of mention for each law enforcement personnel. \n",
    "    This includes police officers, detectives, deupties, lieutenants, sergeants, captains, technicians, coroners, investigators, patrolman, and criminalists, \n",
    "    as described in court transcripts.\n",
    "    Be aware that the titles \"Detective\" and \"Officer\" might be used interchangeably.\n",
    "    Be aware that the titles \"Technician\" and \"Tech\" might be used interchangeably.\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Roles and Responses:\"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "def generate_hypothetical_embeddings():\n",
    "    llm = OpenAI()\n",
    "    prompt = PROMPT_TEMPLATE_HYDE\n",
    "\n",
    "    llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "    base_embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    embeddings = HypotheticalDocumentEmbedder(\n",
    "        llm_chain=llm_chain, base_embeddings=base_embeddings\n",
    "    )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `process_single_document` function converts an input document into a vector database of chunks. This function employs Langchain's RecursiveCharacterTextSplitter to split documents into chunks of 500 tokens, while maintaining an overlap of 250 tokens to ensure contextual continuity.\n",
    "\n",
    "There are times when the model might inadvertently identify names without clear ties to law enforcement personnel. By cross-referencing the model's output with LLEAD, we believe we will be able to filter out many such false positives (it's worth noting that some law enforcement personnel mentioned in the documents will be absent from LLEAD, but our current focus is on officers we can track using LLEAD). On the other hand, we have no way of recovering officer mentions if they are not picked up by our extraction process. In light of this, when evaluating the model we are more interested in maximizing recall, ensuring we identify as many genuine law enforcement mentions as we can.  To quantify this focus on recall, we employed the [F-beta score](https://en.wikipedia.org/wiki/F-score#F%CE%B2_score) (with β=2), which weighs recall twice as heavily as precision. We tested the model using varying chunk sizes, including 2000, 1000, and 500, with corresponding overlaps of 1000, 500, and 250 respectively. Based on our evaluations, the optimal configuration is a chunk size of 500 with an overlap of 250. After segmentation, the text is transformed into a high-dimensional space using precomputed embeddings from our hypothetical document embedder. The FAISS.from_documents function aids in this transformation, constructing an indexed document database designed for similarity searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_document(file_path, embeddings):\n",
    "    logger.info(f\"Processing document: {file_path}\")\n",
    "\n",
    "    loader = JSONLoader(file_path)\n",
    "    text = loader.load()\n",
    "    logger.info(f\"Text loaded from document: {file_path}\")\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=250)\n",
    "    docs = text_splitter.split_documents(text)\n",
    "\n",
    "    db = FAISS.from_documents(docs, embeddings)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections, we define the core function get_response_from_query(db, query). This function serves as the backbone of our information extraction process, taking in a document database and a query, and returning the system's response to the query. \n",
    "\n",
    "The process begins by setting up the relevant parameters. We use a prompt template to guide the query and a role template to define the roles we're interested in. We set the temperature parameter to 0 to maximize the determinism of our responses. The k parameter is set to 20, a decision guided by the F-beta score results from our testing phase, instructing the system to select and concatenate the top 20 relevant text chunks from the document corpus. These documents are then sorted by similarity score to maximize the model's performance. As suggested in the paper [Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172), for retrieval tasks current language models perform best when the relevant data is located at the beginning of their context window.\n",
    "\n",
    "The relevant chunks of text are then passed to the LLMChain class of the LangChain module as part of the 'run' method. In addition to relevant chunks, the 'run' method also receives the PromptTemplate, RoleTemplate, and the original query.\n",
    "\n",
    "The LLMChain processes these inputs and generates a structured response to the initial query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE_MODEL = PromptTemplate(\n",
    "    input_variables=[\"roles\" ,\"question\", \"docs\"],\n",
    "    template=\"\"\"\n",
    "    As an AI assistant, my role is to meticulously analyze court transcripts, traditional officer roles, and extract information about law enforcement personnel.\n",
    "\n",
    "    Query: {question}\n",
    "\n",
    "    Transcripts: {docs}\n",
    "\n",
    "    Roles: {roles}\n",
    "\n",
    "    The response will contain:\n",
    "\n",
    "    1) The name of a officer, detective, deputy, lieutenant, \n",
    "       sergeant, captain, officer, coroner, investigator, criminalist, patrolman, or technician - \n",
    "       if an individual's name is not associated with one of these titles they do not work in law enforcement.\n",
    "       Please prefix the name with \"Officer Name: \". \n",
    "       For example, \"Officer Name: John Smith\".\n",
    "\n",
    "    2) If available, provide an in-depth description of the context of their mention. \n",
    "       If the context induces ambiguity regarding the individual's employment in law enforcement, \n",
    "       remove the individual.\n",
    "       Please prefix this information with \"Officer Context: \". \n",
    "\n",
    "    3) Review the context to discern the role of the officer.\n",
    "       Please prefix this information with \"Officer Role: \"\n",
    "       For example, the column \"Officer Role: Lead Detective\" will be filled with a value of 1 for officer's who were the lead detective.\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "ROLE_TEMPLATE = \"\"\"\n",
    "US-IPNO-Exonerations: Model Evaluation Guide \n",
    "Roles:\n",
    "Lead Detective\n",
    "•\tCoordinates with other detectives and law enforcement officers on the case.\n",
    "•\tLiaises with the prosecutor's office, contributing to legal strategy and court proceedings.\n",
    "\n",
    "Crime Lab Analyst:\n",
    "•  Analyses various types of evidence gathered during an investigation, including but not limited to, DNA, fingerprints, blood samples, drug substances, etc.\n",
    "•  Prepares detailed reports outlining the findings of their analyses.\n",
    "\"\"\"\n",
    "\n",
    "def get_response_from_query(db, query):\n",
    "    # Set up the parameters\n",
    "    prompt = PROMPT_TEMPLATE_MODEL\n",
    "    roles = ROLE_TEMPLATE\n",
    "    temperature = 0\n",
    "    k = 20\n",
    "\n",
    "    # Perform the similarity search\n",
    "    doc_list = db.similarity_search_with_score(query, k=k)\n",
    "\n",
    "    # Sort documents by relevance scores. Place documents with the highest relevance \n",
    "\n",
    "    docs = sorted(doc_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    third = len(docs) // 3\n",
    "\n",
    "    highest_third = docs[:third]\n",
    "    middle_third = docs[third:2*third]\n",
    "    lowest_third = docs[2*third:]\n",
    "\n",
    "    highest_third = sorted(highest_third, key=lambda x: x[1], reverse=True)\n",
    "    middle_third = sorted(middle_third, key=lambda x: x[1], reverse=True)\n",
    "    lowest_third = sorted(lowest_third, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    docs = highest_third + lowest_third + middle_third\n",
    "\n",
    "    docs_page_content = \" \".join([d[0].page_content for d in docs])\n",
    "\n",
    "    # Create an instance of the OpenAI model\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "    # Create an instance of the LLMChain\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "    # Run the LLMChain and print the response\n",
    "    response = chain.run(roles=roles, question=query, docs=docs_page_content, temperature=temperature)\n",
    "    print(response)\n",
    "\n",
    "    # Return the response and the documents\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For additional context, see the following inputs and outputs:\n",
    "\n",
    "**Query**\n",
    "\n",
    "\"Identify individuals, by name, with the specific titles of officers, sergeants, lieutenants, captains, detectives, homicide officers, and crime lab personnel in the transcript. Specifically, provide the context of their mention related to key events in the case, if available.\"\n",
    "\n",
    "**Relevant Document**\n",
    "\n",
    "(1 of 20 documents identified by the Faiss similarity search as relevant)\n",
    " \n",
    " Martin Venezia, New Orleans police sergeant. A 16 .01 Sergeant Venezia, where are you assigned now? : - A Second Police District. 13 . And in October, September of 1979 and in Q 19 September and October of 1980, where were you assigned? :1 Homicide division. A. And how long have you been on the police department right now? Thirteen and a half years. A Officer Venezia, when did you or did you ever take over the investigation of ... Cathy Ulfers' murder? A\", metadata={'source': '../../data/convictions/transcripts/iterative\\\\(C) Det. Martin Venezia Testimony - Trial One.docx'\n",
    "\n",
    "**Response from the Model** \n",
    "\n",
    "Officer Name: Sergeant Martin Venezia\n",
    "\n",
    "Officer Context: Sergeant Martin Venezia, formerly assigned to the Homicide Division, took over the investigation of Cather Ulfers murder.\n",
    "\n",
    "Officer Role: Lead Detective "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluations-issues-improvements\"></a>\n",
    "# Evaluations, issues, improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our effort to optimize the model's capability to extract officer names from documents, we evaluated it on various parameters. The following tests were run using GPT-4.\n",
    "\n",
    "**Preprocessing Parameters**:\n",
    "\n",
    "- Chunk Size: Defines the number of consecutive words or units of text processed at once.\n",
    "- Chunk Overlap: This parameter dictates the number of words consecutive chunks share. For example, with an overlap of 250, the subsequent chunk begins 250 words into the previous chunk.\n",
    "\n",
    "**Model-specific Parameters**:\n",
    "\n",
    "- Hypothetical Document Embeddings (HYDE): Investigated their effect on the model's overall performance.\n",
    "- 'k' Value: Denotes the number of text chunks input to the model.\n",
    "- Temperature Parameter: Influences the randomness of the model.\n",
    "\n",
    "For evaluating our model's performance, we utilized the F-beta score as our primary metric. Unlike the F1 score, which gives equal weight to precision (correctness) and recall (completeness), the F-beta score allows for differential weighting. We designed our score to weigh recall twice as much as precision, reflecting the importance of accurately spotting relevant information, even if it means occasionally flagging some irrelevant content.\n",
    "\n",
    "Based on our evaluations, our model performed best with:\n",
    "- A chunk size of 500\n",
    "- A chunk overlap of 250\n",
    "- Incorporating HYDE embeddings\n",
    "- A 'k' value of 20\n",
    "\n",
    "For police reports, the F-beta score reached 0.864909, while for transcripts, the F-beta score peaked at 0.813397.\n",
    "\n",
    "Although larger chunk sizes, such as 1000 and 2000, might offer advantages for certain applications, they resulted in lower F-beta scores during our tests. Similarly, greater overlaps like 500 and 1000 reduced our performance, even with the potential for more context. The consistent advantage of incorporating HYDE embeddings was evident, underscoring their value to our model.\n",
    "\n",
    "Another key observation was regarding the temperature parameter, which influences the model's level of randomness. With a temperature set to 1, we generally saw higher F-beta scores, especially for identifying officer names in police reports. As we move to the next phase — extracting detailed context about the officers role within the document — the precise handling of this parameter will be crucial because a high temperature can potentially skew results or generate \"hallucinated\" content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>chunk_overlap</th>\n",
       "      <th>temperature</th>\n",
       "      <th>k</th>\n",
       "      <th>hyde</th>\n",
       "      <th>filetype</th>\n",
       "      <th>FN</th>\n",
       "      <th>FP</th>\n",
       "      <th>TP</th>\n",
       "      <th>n_files</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F_beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500</td>\n",
       "      <td>250</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>police-report</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>105</td>\n",
       "      <td>5</td>\n",
       "      <td>0.981308</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.905172</td>\n",
       "      <td>0.864909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>police-report</td>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "      <td>71</td>\n",
       "      <td>5</td>\n",
       "      <td>0.689320</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.763441</td>\n",
       "      <td>0.816092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500</td>\n",
       "      <td>250</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>transcript</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>0.557377</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.693878</td>\n",
       "      <td>0.813397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>police-report</td>\n",
       "      <td>6</td>\n",
       "      <td>56</td>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.659341</td>\n",
       "      <td>0.789474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>police-report</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>54</td>\n",
       "      <td>5</td>\n",
       "      <td>0.805970</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.787172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>transcript</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.787037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1000</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>transcript</td>\n",
       "      <td>15</td>\n",
       "      <td>31</td>\n",
       "      <td>57</td>\n",
       "      <td>6</td>\n",
       "      <td>0.647727</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.712500</td>\n",
       "      <td>0.757979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>transcript</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.738916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>police-report</td>\n",
       "      <td>13</td>\n",
       "      <td>37</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>0.569767</td>\n",
       "      <td>0.790323</td>\n",
       "      <td>0.662162</td>\n",
       "      <td>0.733533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1000</td>\n",
       "      <td>500</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>police-report</td>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>78</td>\n",
       "      <td>5</td>\n",
       "      <td>0.886364</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.727612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>police-report</td>\n",
       "      <td>37</td>\n",
       "      <td>19</td>\n",
       "      <td>86</td>\n",
       "      <td>5</td>\n",
       "      <td>0.819048</td>\n",
       "      <td>0.699187</td>\n",
       "      <td>0.754386</td>\n",
       "      <td>0.720268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>500</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>transcript</td>\n",
       "      <td>19</td>\n",
       "      <td>29</td>\n",
       "      <td>53</td>\n",
       "      <td>6</td>\n",
       "      <td>0.646341</td>\n",
       "      <td>0.736111</td>\n",
       "      <td>0.688312</td>\n",
       "      <td>0.716216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1000</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>police-report</td>\n",
       "      <td>13</td>\n",
       "      <td>70</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "      <td>0.465649</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.595122</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>transcript</td>\n",
       "      <td>44</td>\n",
       "      <td>36</td>\n",
       "      <td>50</td>\n",
       "      <td>9</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.541126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1000</td>\n",
       "      <td>500</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>transcript</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>0.372549</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.441860</td>\n",
       "      <td>0.497382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    chunk_size  chunk_overlap  temperature   k  hyde       filetype  FN  FP  \\\n",
       "4          500            250            1  20     1  police-report  20   2   \n",
       "2         2000           1000            1   5     0  police-report  12  32   \n",
       "0          500            250            1  20     1     transcript   3  27   \n",
       "1          500            250            0  20     1  police-report   6  56   \n",
       "8         2000           1000            0   5     0  police-report  15  13   \n",
       "3         2000           1000            1   5     1     transcript   3  11   \n",
       "6         1000            500            0  10     1     transcript  15  31   \n",
       "10        2000           1000            0   5     1     transcript  22  18   \n",
       "7         2000           1000            0   5     1  police-report  13  37   \n",
       "12        1000            500            1  10     1  police-report  34  10   \n",
       "11        2000           1000            1   5     1  police-report  37  19   \n",
       "9          500            250            0  20     1     transcript  19  29   \n",
       "5         1000            500            0  10     1  police-report  13  70   \n",
       "14        2000           1000            0   5     0     transcript  44  36   \n",
       "13        1000            500            1  10     1     transcript  16  32   \n",
       "\n",
       "     TP  n_files  precision    recall        F1    F_beta  \n",
       "4   105        5   0.981308  0.840000  0.905172  0.864909  \n",
       "2    71        5   0.689320  0.855422  0.763441  0.816092  \n",
       "0    34        4   0.557377  0.918919  0.693878  0.813397  \n",
       "1    60        5   0.517241  0.909091  0.659341  0.789474  \n",
       "8    54        5   0.805970  0.782609  0.794118  0.787172  \n",
       "3    17        3   0.607143  0.850000  0.708333  0.787037  \n",
       "6    57        6   0.647727  0.791667  0.712500  0.757979  \n",
       "10   60        7   0.769231  0.731707  0.750000  0.738916  \n",
       "7    49        5   0.569767  0.790323  0.662162  0.733533  \n",
       "12   78        5   0.886364  0.696429  0.780000  0.727612  \n",
       "11   86        5   0.819048  0.699187  0.754386  0.720268  \n",
       "9    53        6   0.646341  0.736111  0.688312  0.716216  \n",
       "5    61        5   0.465649  0.824324  0.595122  0.714286  \n",
       "14   50        9   0.581395  0.531915  0.555556  0.541126  \n",
       "13   19        4   0.372549  0.542857  0.441860  0.497382  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_summary():\n",
    "    summary = pd.read_excel(\"data/overall-summary-with-F1-Fbeta.xlsx\")\n",
    "    summary = summary.sort_values(\"F_beta\", ascending=False)\n",
    "    return summary\n",
    "\n",
    "read_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After our evaluation of the model based on various parameters, the next phase delved into understanding the model's behavior over iterative runs. Due to the stochastic nature of generative text models, a single document can yield diverse outputs when processed multiple times using the same parameters. This highlighted the challenge of identifying an optimal number of iterations, a balance that ensures comprehensive insights while being cost efficient. In the interest of cost efficiency, the following tests were run using GPT-3.5-Turbo-16K. The decline in performance can be attributed to the change in model. \n",
    "\n",
    "To address this, we employed two distinct query strategies:\n",
    "\n",
    "Multiple Queries Approach: This strategy used six unique queries, each crafted to extract specific facets of the required information. The queries are as follows:\n",
    "\n",
    "- Query 1: Identify individuals, by name, with the specific titles of officers, sergeants, lieutenants, captains, detectives, homicide officers, and crime lab personnel in the transcript. Specifically, provide the context of their mention related to key events in the case, if available.\n",
    "\n",
    "- Query 2: List individuals, by name, directly titled as officers, sergeants, lieutenants, captains, detectives, homicide units, and crime lab personnel mentioned in the transcript. Provide the context of their mention in terms of any significant decisions they made or actions they took.\n",
    "\n",
    "- Query 3: Locate individuals, by name, directly referred to as officers, sergeants, lieutenants, captains, detectives, homicide units, and crime lab personnel in the \n",
    "transcript. Explain the context of their mention in relation to their interactions with other individuals in the case.\"\n",
    "\n",
    "- Query 4: Highlight individuals, by name, directly titled as officers, sergeants, lieutenants, captains, detectives, homicide units, and crime lab personnel in the transcript. Describe the context of their mention, specifically noting any roles or responsibilities they held in the case.\n",
    "\n",
    "- Query 5: Outline individuals, by name, directly identified as officers, sergeants, lieutenants, captains, detectives, homicide units, and crime lab personnel in the transcript. Specify the context of their mention in terms of any noteworthy outcomes or results they achieved.\n",
    "\n",
    "- Query 6: Pinpoint individuals, by name, directly labeled as officers, sergeants, lieutenants, captains, detectives, homicide units, and crime lab personnel in the transcript. Provide the context of their mention, particularly emphasizing any significant incidents or episodes they were involved in.\n",
    "\n",
    "Singular Query Approach: This method employed a comprehensive query, designed to holistically capture all the desired information facets. We run the document through the same query repeatedly, which results in slightly different responses each time, and then collect the results together. The query is:\n",
    "\n",
    "- Query: Identify each individual in the transcript, by name, who are directly referred to as officers, sergeants, lieutenants, captains, detectives, homicide officers, and crime lab personnel. Provide the context of their mention, focusing on key events, significant decisions or actions they made, interactions with other individuals, roles or responsibilities they held, noteworthy outcomes or results they achieved, and any significant incidents or episodes they were involved in, if available.\n",
    "\n",
    "\n",
    "In the context of police reports, a detailed analysis of the singular query method showed that its performance metrics improved with each iteration up to the 4th iteration, with a marked increase in the F1 Beta score after each iteration. However, past this iteration, the incremental gains in the score reduced, indicating we had hit the point of diminishing returns.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"https://github.com/ayyubibrahimi/llm-criminal-justice-research/assets/57520778/5eb5aff0-26b9-4fab-934b-9dbd5195b626\" alt=\"Figure 1: Average Cumulative F1 Beta Score for 1 Query vs. 6 Queries (Police Reports)\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "\n",
    "The analysis of court transcripts, on the other hand, offered a nuanced perspective. Both the singular and the 6 queries methods exhibited an upward trend in their performance metrics up until the 6th iteration.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"https://github.com/ayyubibrahimi/llm-criminal-justice-research/assets/57520778/bc5e49c7-b510-40e1-a410-2c2372b3928e\" alt=\"Figure 2: Average Cumulative F1 Beta Score for 1 Query vs. 6 Queries (Transcripts)\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "\n",
    "Analyzing the results from both police reports and court transcripts gave us confidence in the singular query method. It consistently demonstrated a balance between performance and computational/cost demands, with diminishing performance gains beyond the 4th iteration in both datasets. Therefore, based on this analysis, the singular query strategy was selected for deployment over 4 iterations for all types of documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"fine-tuning\"></a>\n",
    "# Fine-tuning\n",
    "\n",
    "Currently, GPT-4's pricing is 0.03 per 1K tokens for inputs and 0.06 per 1K tokens for outputs. In contrast, GPT-3-Turbo-4K is priced at 0.0015 per 1K tokens for inputs and 0.002 per 1K tokens for outputs. This means using GPT-4 is 19x and 29x more expensive, respectively. Given these cost considerations, coupled with challenges our existing GPT-3.5-Turbo-16K model faced in extracting officer details from documents in our FAISS similarity database, we've pivoted our focus towards the GPT-3.5-Turbo-4K model. While the GPT-3.5-Turbo-4K itself isn't new, the capability to fine-tune it was introduced by OpenAI in August 2023, offering a promising avenue for improvement.\n",
    "\n",
    "In order to address our model's shortcomings, we found ways to efficiently generate additional training data. Using document samples, we extracted details about individual law enforcement officers, their contexts, and roles. Recognizing the potential of GPT-4, we leveraged its capabilities to craft training documents that closely resembled our real-world challenges. We provided GPT-4 with sample documents based on authentic data, enabling it to produce outputs with the stylistic nuances we often encounter—like poor OCR quality, fragmented sentences, inconsistent capitalization, and syntactic inconsistencies.\n",
    "\n",
    "Here's an example of the training data we generated using GPT-4:\n",
    "\n",
    "{ \n",
    "    \n",
    "\"messages\": \n",
    "\n",
    "[{ \"role\": \"system\", \"content\": \n",
    "- \"As an AI assistant, my role is to meticulously analyze criminal justice documents and extract information about law enforcement personnel. The response will contain: 1) The name of a law enforcement personnel. The individual's name must be prefixed with one of the following titles to be in law enforcement: Detective, Sergeant, Lieutenant, Captain, Deputy, Officer, Patrol Officer, Criminalist, Technician, Coroner, or Dr. Please prefix the name with 'Officer Name:  I will derive this data from the following paragraph: On September 13, DET. X. Y. Allen responded to claims of counterfeit money circulating in the Westside Market. Primary informants were Mrs. Jacobs, a vendor, and Mr. Silva, a customer. FORWARD COPY TO: DETECTIVE DIVISION. INFORMANT DOCS.\"}, \n",
    "\n",
    "{ \"role\": \"user\", \"content\": \n",
    "- 'Identify each individual in the transcript, by name, who are directly referred to as officers, sergeants, lieutenants, captains, detectives, homicide officers, and crime lab personnel.\" }, \n",
    "\n",
    "{ \"role\": \"assistant\", \"content\": \n",
    "- \"Officer Name: DET. X. Y. Allen\n",
    "- Officer Context: On September 13, DET. X. Y. Allen responded to claims of counterfeit money circulating in the Westside Market. \n",
    "- Officer Role: Detective\" }] \n",
    "\n",
    "}\n",
    "\n",
    "In our fine-tuning experiments, we worked with four dataset sizes: 25, 50, 75, and 100 examples. Analyzing the outcomes, a clear trend emerged: as we increased the dataset size, the model's performance improved incrementally. Even with the constraints of the 4k token limit, which led us to adjust our K parameter from 25 to 15, our model exhibited differentiated performance across document types. It surpassed the GPT-3.5-Turbo-16k model when processing court transcripts and matched its efficiency for police reports. However, as promising as these strides are, they haven't yet reached the capabilities of GPT-4 (See appendix for GPT-4 results).\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"https://github.com/ayyubibrahimi/llm-criminal-justice-research/assets/57520778/c0f108c5-c202-4887-ba77-997b1e6575dd\" alt=\"Figure 3: Average Cumulative F1 Beta Score for 1 Query vs. 6 Queries (Police Reports) - Model: GPT-3\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"https://github.com/ayyubibrahimi/llm-criminal-justice-research/assets/57520778/90a78aab-f358-4947-aa82-43f95dea568e\" alt=\"Figure 4: Average Cumulative F1 Beta Score for 1 Query vs. 6 Queries (Court Transcripts) - Model: GPT-3\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "The results from our current fine-tuning experiments with GPT models provide valuable insights into the potential and limitations of AI in data extraction tasks. Our observations underscore the significance of dataset size and quality, as well as the implications of token constraints on model performance. As we move forward:\n",
    "\n",
    "We will delve deeper into the interplay between token limits and extraction accuracy, particularly in documents with varying complexities.\n",
    "We'll further investigate the optimal balance between training data volume and model efficiency, exploring potential diminishing returns or inflection points.\n",
    "Given the differentiated performance across document types, our research will also focus on domain-specific fine-tuning to optimize extraction from court transcripts, police reports, and other legal documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Complex Tree of Thought Methodology and Its Integration with Fine-Tuned GPT-3-Turbo\n",
    "\n",
    "The Tree of Thought methodology, as implemented in our work, stands as a structured approach to problem-solving within the realms of AI and machine learning. This methodology aligns with the natural cognitive process of humans, where complex tasks are broken down into simpler, more manageable components. Such an approach is especially effective in parsing and processing extensive and intricate queries often encountered in legal and technical domains.\n",
    "\n",
    "In large language models (LLMs), like GPT-3-Turbo, the Tree of Thought methodology plays a pivotal role. It allows for the segmentation of complex queries into smaller, distinct elements, enabling the model to address each part with increased focus and precision. This leads to more accurate and comprehensive responses, a crucial factor in fields requiring detailed and precise information extraction.\n",
    "\n",
    "Building on Fine-Tuning GPT-3-Turbo\n",
    "Our approach takes a significant step forward by building on the fine-tuned GPT-3-Turbo. Fine-tuning this advanced model has enabled us to tailor its capabilities to our specific requirements, particularly in the context of legal document analysis. Fine-tuning involves adjusting the model on a dataset that is representative of the specific tasks it will perform, allowing it to understand and process queries with a higher degree of accuracy and relevance to the given context.\n",
    "\n",
    "By integrating the Tree of Thought methodology with a fine-tuned GPT-3-Turbo, we have achieved several key advancements:\n",
    "\n",
    "- Enhanced Precision and Relevance: The fine-tuned model is more adept at handling the nuances of legal language and the specificities of complex queries, leading to outputs that are highly relevant and precise.\n",
    "\n",
    "- Improved Problem Decomposition: The fine-tuning allows the model to better decompose complex queries into manageable segments, aligning with the Tree of Thought approach for systematic problem-solving.\n",
    "\n",
    "- Adaptive Query Processing: The model can adaptively process each segment of a query, considering the unique aspects and requirements of each part, thereby improving the overall quality and applicability of the responses.\n",
    "\n",
    "In summary, the integration of the Tree of Thought methodology with fine-tuned GPT-3-Turbo marks a significant leap in our ability to effectively process and analyze complex queries in legal document analysis. This combination harnesses the strengths of both advanced model tuning and structured problem-solving approaches, setting a new standard in the field of AI-driven legal analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Application in Document Relevance Evaluation**\n",
    "\n",
    "In the realm of legal document analysis, the application of the Tree of Thought methodology significantly enhances the model's capability to evaluate document relevance. This process begins with the LLM assessing the overall relevance of a set of documents to a given query. Instead of attempting to process the entire query and all documents in one go, the methodology advocates for a staggered approach. Each document is examined in the context of specific query elements, allowing the model to meticulously analyze and score its relevance. This granular approach to relevance evaluation not only increases the precision of the model but also enables it to handle complex legal language and concepts more effectively. The scoring system, based on a scale of 1-10, provides a quantitative measure of relevance, assisting in the objective assessment of each document's utility in answering the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_document_relevance(llm, docs, query, threshold):\n",
    "    template = \"\"\"\n",
    "    Documents: {docs}\n",
    "    Query: {query}\n",
    "    \n",
    "    As an AI assistant, your task is to evaluate the relevance and quality of the documents with respect to their ability to provide an answer to the query. \n",
    "   \n",
    "    Provide a score on a scale from 1-10. \n",
    "    Povide one score, for all the documents, in the following format: \n",
    "    \n",
    "    confidence_score: (score)\n",
    "\n",
    "    Only provide the score with the prefix \"confidence_score: \"\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(input_variables=[\"docs\", \"query\"], template=template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt, output_key=\"confidence_score\")\n",
    "    result = chain.run(docs=docs, query=query, temperature=0)\n",
    "\n",
    "    if ':' in result:\n",
    "        result_dict = {\"confidence_score\": float(result.split(\":\")[1].strip())}\n",
    "    else:\n",
    "        try:\n",
    "            result_dict = {\"confidence_score\": float(result.strip())}\n",
    "        except ValueError:\n",
    "            # Handle the case where the result is not a float\n",
    "            logging.warning(f\"Could not convert result to a float: {result}\")\n",
    "            result_dict = {\"confidence_score\": 0}\n",
    "        # print(f\"Query: {query}, Result: {result}\")\n",
    "\n",
    "    try:\n",
    "        confidence_score = int(result_dict[\"confidence_score\"])\n",
    "    except ValueError:\n",
    "        logging.warning(\n",
    "            f\"Could not convert confidence score to an integer: {result_dict['confidence_score']}\"\n",
    "        )\n",
    "        confidence_score = 0\n",
    "\n",
    "    better_query_needed = confidence_score < threshold\n",
    "\n",
    "    return {\n",
    "        \"confidence_score\": confidence_score,\n",
    "        \"better_query_needed\": better_query_needed,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Query Refinement**\n",
    "\n",
    "The query refinement process is where the Tree of Thought methodology truly shines. Post the initial evaluation of documents, the model employs feedback mechanisms to iteratively refine the query. If the confidence score of the documents’ relevance falls below a predetermined threshold, the model enters a refinement loop. In this stage, it re-examines the query in light of the documents' content, identifying areas where the query could be more specific or aligned with the information available in the documents. This iterative process ensures that the refined query is more targeted and likely to yield relevant documents. By continually adjusting the query based on the documents' analysis, the model adapts to the intricacies and nuances of legal language, enhancing its efficiency and accuracy in document retrieval and information extraction. The refined query is then used in subsequent searches within the database, exemplifying a dynamic and responsive approach to legal document analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_better_query(llm, original_query, docs, threshold):\n",
    "    # Template for generating a better query\n",
    "    template = \"\"\"\n",
    "    Transcripts: {docs}\n",
    "    The original query: {original_query} did not yield satisfactory results with a confidence below {threshold}.\n",
    "    Please provide a new and improved query that we can send to the faiss vector database for document retrival.\n",
    "    The new query must aim to answer the {original_query}\n",
    "    A:\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"original_query\", \"docs\", \"threshold\"], template=template\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt, output_key=\"better_query\")\n",
    "    result = chain.run(\n",
    "        original_query=original_query, docs=docs, threshold=threshold, temperature=0\n",
    "    )\n",
    "    if result:\n",
    "        better_query = result\n",
    "    else:\n",
    "        logging.warning(\"Result is empty. Using original query instead.\")\n",
    "        better_query = original_query\n",
    "    return better_query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Brainstorming Stage: Strategy Development**\n",
    "\n",
    "The brainstorming stage in your model plays a crucial role in developing comprehensive strategies for extracting data from law enforcement documents. This stage involves generating multiple strategic approaches for parsing the documents to identify all law enforcement personnel mentioned, along with their contexts and roles. The Large Language Model (LLM) is prompted to brainstorm three distinct strategies, ensuring each one aims to minimize ambiguity and maximizes the accuracy and comprehensiveness of the extracted information.\n",
    "\n",
    "This process is not just about generating potential solutions; it's about creatively leveraging the unique content and context within the documents. The model considers the specific nuances of the text, such as the way officers are referred to, the nature of their involvement in events, and how their roles are described. This approach enables the model to tailor its strategies to the specific characteristics of the dataset, ensuring that the information extraction is as relevant and accurate as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chaining Outputs for Enhanced Analysis**\n",
    "\n",
    "Once the brainstorming stage is complete, the model enters a critical phase where it chains the outputs of various stages to refine its analysis further. This is done in several steps:\n",
    "\n",
    "Evaluating Brainstormed Strategies: The model evaluates the brainstormed strategies for their relevance and effectiveness in answering the query. It assigns a confidence score to each strategy, providing a quantitative measure of its potential utility.\n",
    "\n",
    "Selecting the Best Strategy: Based on the confidence scores, the model selects the most promising strategy. This strategy is then used to guide the subsequent data extraction process. The selection is made by comparing the confidence scores and choosing the one with the highest rating.\n",
    "\n",
    "Data Extraction Based on Selected Strategy: The model uses the chosen strategy to extract detailed information about law enforcement personnel from the documents. It identifies each officer, provides context for their mention, and discerns their specific roles.\n",
    "\n",
    "Re-evaluation for Comprehensive Extraction: Finally, the model re-evaluates the documents to identify any law enforcement personnel it might have missed in the first iteration. This step ensures that the extraction is exhaustive and no relevant information is overlooked.\n",
    "\n",
    "Through these stages, your model demonstrates an advanced level of reasoning and adaptability. It not only devises multiple strategies for tackling a complex task but also intelligently selects the most suitable one based on its potential effectiveness. This methodical and iterative approach enhances the model's accuracy and reliability in extracting and analyzing data from intricate legal documents, ensuring a thorough and nuanced understanding of the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indepth_response_from_query(\n",
    "    db,\n",
    "    query,\n",
    "    TEMPERATURE,\n",
    "    k,\n",
    "    max_iterations=1,\n",
    "):\n",
    "    logger.info(\"Performing in-depth summary query...\")\n",
    "\n",
    "    tuned_model = \"ft:gpt-3.5-turbo-1106:personal::8IOkEZB9\"\n",
    "\n",
    "    llm = tuned_model\n",
    "\n",
    "    iteration_counter_db = 0\n",
    "    confidence_rating_db = 0\n",
    "    threshold_db = 7\n",
    "\n",
    "    ## Stage 1: Query refinement stage. Task: evaluate the relevance of docs returned from vector db with respect to query\n",
    "    best_query = refine_query(db, llm, query, k, threshold_db, sort_retrived_documents)\n",
    "    # print(f\"BEST QUERY: {best_query}\")\n",
    "    docs, docs_page_content, page_numbers = get_response_from_query(db, best_query, 1, k)\n",
    "\n",
    "    # print(best_query_vector)\n",
    "\n",
    "    ## Helper funcs\n",
    "    def execute_brainstorming_stage(docs):\n",
    "        template1 = \"\"\"\n",
    "            Documents: {docs}\n",
    "            Question: {question}\n",
    "\n",
    "            The primary objective is to extract comprehensive data on each and every law enforcement personnel mentioned in the documents provided. This includes their names, the context of their mentions, and their roles within the law enforcement agency.\n",
    "\n",
    "            Brainstorm three distinct strategies that meticulously leverage the specific content and context of these documents to accurately extract the names, context mentions, and roles of all law enforcement personnel mentioned. Each strategy should aim at minimizing ambiguity and ensuring a comprehensive extraction of information.\n",
    "\n",
    "            Please provide the output in the following format:\n",
    "            1. Strategy One:\n",
    "            2. Strategy Two: \n",
    "            3. Strategy Three:\n",
    "\n",
    "            A:\n",
    "            \"\"\"\n",
    "        prompt1 = PromptTemplate(\n",
    "            input_variables=[\"question\", \"docs\"], template=template1\n",
    "        )\n",
    "        chain1 = LLMChain(llm=llm, prompt=prompt1, output_key=\"angles\")\n",
    "        responses_llm = chain1.run(question=best_query, docs=docs, temperature=1)\n",
    "        parsed_angles = parse_angles(responses_llm)\n",
    "        return parsed_angles\n",
    "\n",
    "    def evaluate_output(angles, docs, angle_confidence_dict):\n",
    "        template1_evaluation = \"\"\"\n",
    "            Transcripts: {docs}\n",
    "            Query: {question}\n",
    "            Based on the brainstormed angles: {angles}, rate your confidence in the quality and relevance of these perspectives for answering the query \n",
    "            on a scale of 1 to 10. Only return the confidence score.\n",
    "            A:\n",
    "            \"\"\"\n",
    "        prompt1_evaluation = PromptTemplate(\n",
    "            input_variables=[\"question\", \"docs\", \"angles\"],\n",
    "            template=template1_evaluation,\n",
    "        )\n",
    "        chain1_evaluation = LLMChain(\n",
    "            llm=llm, prompt=prompt1_evaluation, output_key=\"confidence_rating\"\n",
    "        )\n",
    "\n",
    "        for angle, content in angles.items():\n",
    "            result = chain1_evaluation.run(\n",
    "                question=best_query, docs=docs, angles=content, temperature=0\n",
    "            )\n",
    "            print(f\"Angle: {angle}, Result: {result}\")\n",
    "\n",
    "            if isinstance(result, (int, float)):\n",
    "                confidence_rating = result\n",
    "            elif isinstance(result, str):\n",
    "                try:\n",
    "                    confidence_rating = float(result.strip())\n",
    "                except ValueError:\n",
    "                    confidence_rating = 0\n",
    "            else:\n",
    "                confidence_rating = 0\n",
    "\n",
    "            angle_confidence_dict[content] = confidence_rating\n",
    "\n",
    "        # Check if any angle has a confidence score\n",
    "        if not angle_confidence_dict or all(\n",
    "            v == 0 for v in angle_confidence_dict.values()\n",
    "        ):\n",
    "            logging.warning(\n",
    "                \"No angles were evaluated or all angles have zero confidence. Returning the first angle.\"\n",
    "            )\n",
    "            best_angle = list(angles.keys())[0]  # Get the first angle\n",
    "            return {\"best_angle\": best_angle, \"confidence_rating\": 0}\n",
    "\n",
    "        # Sorting the dictionary by values. In case of a tie, the first item with the maximum value will be chosen.\n",
    "        best_angle = max(angle_confidence_dict, key=angle_confidence_dict.get)\n",
    "        print(f\"Best Angle: {best_angle}\")\n",
    "\n",
    "        return {\n",
    "            \"best_angle\": best_angle,\n",
    "            \"confidence_rating\": angle_confidence_dict[best_angle],\n",
    "            \"angle_confidence_dict\": angle_confidence_dict,\n",
    "        }\n",
    "\n",
    "    ### Stage 2: Evaluate angles returned. Choose the best angle.\n",
    "\n",
    "    threshold_brainstorm = 7\n",
    "    iteration_counter_brainstorm = 0\n",
    "    confidence_rating_brainstorm = 0\n",
    "\n",
    "    angle_confidence_dict = {}\n",
    "    while (\n",
    "        confidence_rating_brainstorm < threshold_brainstorm\n",
    "        and iteration_counter_brainstorm < max_iterations\n",
    "    ):\n",
    "        logging.info(\"Brainstorming function invoked.\")\n",
    "        angles_dict = execute_brainstorming_stage(docs_page_content)\n",
    "        response = evaluate_output(angles_dict, docs, angle_confidence_dict)\n",
    "        confidence_rating_brainstorm = int(response.get(\"confidence_rating\", 0))\n",
    "        angle_confidence_dict.update(\n",
    "            response.get(\"angle_confidence_dict\", {})\n",
    "        )  # Cumulatively updating the dictionary\n",
    "\n",
    "        iteration_counter_brainstorm += 1\n",
    "        logging.info(\n",
    "            f\"Iteration: {iteration_counter_brainstorm}, Confidence Rating: {confidence_rating_brainstorm}\"\n",
    "        )\n",
    "\n",
    "    if iteration_counter_brainstorm == max_iterations:\n",
    "        logging.warning(\n",
    "            f\"Maximum number of iterations ({max_iterations}) reached without crossing the confidence threshold. Brainstorm func will no longer be re-run.\"\n",
    "        )\n",
    "\n",
    "    best_angle = max(angle_confidence_dict, key=angle_confidence_dict.get)\n",
    "    print(f\"Best Angle: {best_angle}\")\n",
    "\n",
    "    # Stage 3: Extract Data\n",
    "    template2 = \"\"\"\n",
    "    Question: {question}\n",
    "\n",
    "    Documents: {docs}\n",
    "\n",
    "    Approach to answering query: {angle}\n",
    "\n",
    "    As an AI assistant, my role is to identify each law enforcement officer named in the documents. \n",
    "\n",
    "    For each officer I identify, my response will contain:\n",
    "\n",
    "    The name of a law enforcement personnel. The individual's name must be prefixed with one of the following titles to be in law enforcement: \n",
    "        Detective, Sergeant, Lieutenant, Captain, Deputy, Officer, Patrol Officer, Criminalist, Technician, Coroner, or Dr. \n",
    "        Please prefix the name with \"Officer Name: \". \n",
    "        For example, \"Officer Name: John Smith\".\n",
    "\n",
    "    Provide an in-depth description of the context of their mention. \n",
    "        If the context induces ambiguity regarding the individual's employment in law enforcement, please make this clear in your response.\n",
    "        Please prefix this information with \"Officer Context: \". If the context is not available, still include the officer.\n",
    "\n",
    "    Review the context to discern the role of the officer. For example, Lead Detective (Homicide Division), Supervising Officer (Crime Lab), Detective, Officer on Scene, Arresting Officer, Crime Lab Analyst\n",
    "        Please prefix this information with \"Officer Role: \"\n",
    "        For example, \"Officer Role: Lead Detective\". If the role is not available, still include the officer.\n",
    "        \"\"\"\n",
    "\n",
    "    prompt2 = PromptTemplate(\n",
    "        input_variables=[\"question\", \"docs\", \"angle\"], template=template2\n",
    "    )\n",
    "    chain2 = LLMChain(llm=llm, prompt=prompt2)\n",
    "    response2 = chain2.run(\n",
    "        question=query, docs=docs_page_content, angle=best_angle, temperature=0\n",
    "    )\n",
    "    \n",
    "    # Stage 4 - Re-evaluate\n",
    "    template3 = \"\"\"\n",
    "        Documents: {docs}\n",
    "        Previously extracted law personnel: {first_iteration}\n",
    "        Current task: Re-evaluate to identify law enforcement personnel that you did not extract.\n",
    "\n",
    "        As an AI assistant, my role is to identify each law enforcement officer named in the documents. The individual's name must be prefixed with one of the following titles to be in law enforcement: \n",
    "        Detective, Sergeant, Lieutenant, Captain, Deputy, Officer, Patrol Officer, Criminalist, Technician, Coroner, or Dr. \n",
    "\n",
    "        This iteration will include both the previously extracted law personnel and the newly extracted personnel.\n",
    "\n",
    "        Here are examples of how the data should be formatted.\n",
    "        \n",
    "        Officer Name: Detective Venezia\n",
    "        Officer Context: After completing interviews, Sergeant Ruiz and Detective Venezia decided to separately review the information gathered to date and meet again on the following day to choose the next course of action. \n",
    "        Officer Role: Detective\n",
    "\n",
    "        Officer Name: PO. J. TREADWAY\n",
    "        Officer Context: After a crime was reported, Det. Spong ordered the Crime Lab, and upon arrival of PO. J. Treadway.\n",
    "        Officer Role: Police Officer\n",
    "    \"\"\"\n",
    "\n",
    "    prompt3 = PromptTemplate(\n",
    "        input_variables=[\"docs\", \"first_iteration\"],\n",
    "        template=template3,\n",
    "    )\n",
    "    chain3 = LLMChain(llm=llm, prompt=prompt3)\n",
    "    response = chain3.run(\n",
    "        docs=docs_page_content, first_iteration=response2, temperature=0\n",
    "    )\n",
    "    # print(response)\n",
    "    return response, page_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPT-3.5-Turbo with Complex Tree of Thought Reasoning**\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"https://github.com/ayyubibrahimi/llm-criminal-justice-research/assets/57520778/2ac1a68f-9635-47a9-bb9f-348a481749e9\" alt=\"Figure 5: Average Cumulative F1 Beta Score for Police Reports and Court Transcripts - GPT-3.5-Turbo with Complex Tree of Thought Reasoning\" width=\"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplified Sequential Approach as an Alternative to Complex Tree of Thought Methodology\n",
    "\n",
    "In our research, we developed a simplified sequential approach as an alternative to the more complex Tree of Thought methodology previously employed for legal document analysis. This new approach, while maintaining the essence of the Tree of Thought reasoning, streamlines the process, focusing on iterative review of documents rather than generating new queries, documents, or strategies.\n",
    "\n",
    "Contextualization of the Simplified Approach\n",
    "Our initial complex Tree of Thought methodology involved a comprehensive analysis where the LLM assessed document relevance in stages, examining each document in the context of specific query elements. This granular approach allowed for meticulous analysis and scoring of each document's relevance. The model then iteratively refined the query based on feedback, adapting to the nuances of legal language and the specificities of the documents.\n",
    "\n",
    "Additionally, the model engaged in a brainstorming stage to develop strategies for data extraction, evaluating and selecting the best strategy based on confidence scores. This complex approach, while thorough, involved six API calls and a considerable computational overhead.\n",
    "\n",
    "Rationale for the Simplified Approach\n",
    "The simplified approach was developed in response to findings that the complex model did not generate significant gains through new query generation or document refinement. Instead, a more efficient method was needed that focused on iteratively reviewing previously seen documents to identify missing names.\n",
    "\n",
    "Key Features of the Simplified Approach:\n",
    "\n",
    "Sequential Review: The model follows a three-stage process, each building upon the output of the previous stage. This sequential review ensures that no relevant data is missed, with each stage focusing on extracting and verifying law enforcement personnel data.\n",
    "\n",
    "Conditional Progression: The process advances from one stage to the next based on a confidence score. If the score meets a predetermined threshold (99), the process concludes, thereby avoiding unnecessary computational steps.\n",
    "\n",
    "Reduced API Calls: By limiting the process to three stages, the approach requires only three API calls, significantly reducing computational resources compared to the six calls required in the complex approach.\n",
    "\n",
    "Advantages and Applications\n",
    "This simplified method provides several advantages:\n",
    "\n",
    "Efficiency: The approach is computationally efficient, performing on par with the more complex method while using fewer resources.\n",
    "\n",
    "Focused Analysis: By concentrating on reviewing existing documents, the model effectively identifies all relevant data without the need for extensive query or document refinement.\n",
    "\n",
    "Adaptability: Despite its simplicity, the approach retains adaptability, with each stage informed by the outcomes of the preceding one, a key aspect of the Tree of Thought reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def perform_stage_1(llm, query, docs_page_content):\n",
    "    template1 = \"\"\"\n",
    "    Question: {question}\n",
    "\n",
    "    Documents: {docs}\n",
    "    \n",
    "    As an AI with expertise in analyzing police reports, my objective is to meticulously identify and describe each police officer officer referenced in the provided documents. \n",
    "\n",
    "    For each officer I identify, my response will contain:\n",
    "\n",
    "    The name of a police officer. Police officer names will likely be prefixed with one of the following titles: Detective, Det., Sergeant, Sgt., Lieutenant, Lt., Captain, Cpt., Deputy, Dpt., Officer, Ofc., Patrol Officer, Criminalist, Technician, Coroner.\n",
    "    Please prefix the name with \"Officer Name: \". \n",
    "    For example, \"Officer Name: John Smith\".\n",
    "\n",
    "    Provide an in-depth description of the context of their mention. I will note any ambiguities regarding their law enforcement role.\n",
    "    Please prefix this information with \"Officer Context: \". If the context is not available, I will note this. \n",
    "\n",
    "    Review the context to discern the role of the officer and state their function or position, as indicated in the documents.\n",
    "    Please prefix this information with \"Officer Role: \"\n",
    "\n",
    "    The full response should follow the format below, with no prefixes such as 1., 2., 3., a., b., c., etc.:\n",
    "\n",
    "    Officer Name: John Smith \n",
    "    Officer Context: Mentioned as someone who was present during a search, along with other detectives from different units.\n",
    "    Officer Role: Patrol Officer\n",
    "\n",
    "    Officer Name: \n",
    "    Officer Context:\n",
    "    Officer Role: \n",
    "\n",
    "    Additionally, I will provide a confidence score (from 0 to 100) indicating my certainty in the completeness and accuracy of the extracted names. A higher score indicates higher confidence.\n",
    "\n",
    "    Confidence Score:\n",
    "    \n",
    "    The confidence score should be an integer. \n",
    "    \"\"\"\n",
    "\n",
    "    prompt1 = PromptTemplate(input_variables=[\"question\", \"docs\"], template=template1)\n",
    "    chain1 = LLMChain(llm=llm, prompt=prompt1)\n",
    "    response1 = chain1.run(question=query, docs=docs_page_content, temperature=0)\n",
    "    confidence_score = parse_confidence_score(response1)\n",
    "    return response1, confidence_score\n",
    "\n",
    "\n",
    "def perform_stage_2(llm, docs, first_iteration_response):\n",
    "    template2 = \"\"\"\n",
    "    Previously identified law enforcement personnel from Stage 1: \n",
    "    {first_iteration_names}\n",
    "\n",
    "    In this stage, I will re-evaluate the documents to identify any police officers that I might have missed in the first iteration. It is important to include all names from the first iteration in my response here, along with any new identifications. \n",
    "        \n",
    "    Documents: {docs}\n",
    "\n",
    "    As an AI with expertise in analyzing police reports, my role is to identify each police officer named in the documents, Specifically, I will identify any police officers.\\\n",
    "    Police officer names will be prefixed with one of the following titles: Detective, Sergeant, Lieutenant, Captain, Deputy, Officer, Patrol Officer, Criminalist, Technician, Coroner.\n",
    "\n",
    "\n",
    "    This iteration will include both the previously identified police officers and the newly identified police officers. I will identify officers in the same format as the first iteration, for example:\n",
    "\n",
    "    Officer Name:\n",
    "    Officer Context:\n",
    "    Officer Role:\n",
    "\n",
    "    Officer Name:\n",
    "    Officer Context:\n",
    "    Officer Role:\n",
    "\n",
    "    Officer Name:\n",
    "    Officer Context:\n",
    "    Officer Role:\n",
    "\n",
    "    Do not number or add dashes to the data.\n",
    "\n",
    "    Additionally, I will provide a confidence score (from 0 to 100) indicating my certainty in the completeness and accuracy of the extracted names. A higher score indicates higher confidence.\n",
    "\n",
    "    Confidence Score:\n",
    "    \"\"\"\n",
    "\n",
    "    prompt2 = PromptTemplate(\n",
    "        input_variables=[\"docs\", \"first_iteration_names\"],\n",
    "        template=template2,\n",
    "    )\n",
    "    chain2 = LLMChain(llm=llm, prompt=prompt2)\n",
    "\n",
    "    response2 = chain2.run(\n",
    "        docs=docs, first_iteration_names=first_iteration_response, temperature=0\n",
    "    )\n",
    "    confidence_score = parse_confidence_score(response2)\n",
    "\n",
    "    return response2, confidence_score\n",
    "\n",
    "\n",
    "def perform_stage_3(llm, docs, second_iteration_response):\n",
    "    template3 = \"\"\"\n",
    "        Previously identified law enforcement personnel from Stage 1: \n",
    "        {second_iteration_names}\n",
    "\n",
    "        Documents: {docs}\n",
    "\n",
    "\n",
    "        As an AI with expertise in analyzing police reports, my role is to identify each police officer named in the documents, Specifically, I will identify any police officers.\n",
    "        Police officer names will be prefixed with one of the following titles: Detective, Sergeant, Lieutenant, Captain, Deputy, Officer, Patrol Officer, Criminalist, Technician, Coroner.\n",
    "\n",
    "        This iteration will include both the previously identified law personnel and the newly identified personnel. I will identify officers in the same format as the first iteration, for example:\n",
    "\n",
    "        Officer Name:\n",
    "        Officer Context:\n",
    "        Officer Role:\n",
    "\n",
    "        Officer Name:\n",
    "        Officer Context:\n",
    "        Officer Role:\n",
    "\n",
    "        Officer Name:\n",
    "        Officer Context:\n",
    "        Officer Role:\n",
    "\n",
    "        Do not number or add dashes to the data.\n",
    "    \"\"\"\n",
    "    prompt3 = PromptTemplate(\n",
    "        input_variables=[\"docs\", \"second_iteration_names\"],\n",
    "        template=template3,\n",
    "    )\n",
    "    chain3 = LLMChain(llm=llm, prompt=prompt3)\n",
    "    final_response = chain3.run(\n",
    "        docs=docs, second_iteration_names=second_iteration_response, temperature=0\n",
    "    )\n",
    "\n",
    "\n",
    "    return final_response\n",
    "\n",
    "\n",
    "def get_indepth_response_from_query(db, query, temperature, k):\n",
    "    CONFIDENCE_THRESHOLD = 99\n",
    "    tuned_model = \"ft:gpt-3.5-turbo-1106:personal::8IOkEZB9\"\n",
    "    \n",
    "    llm = ChatOpenAI(model_name=tuned_model)\n",
    "\n",
    "    docs_page_content_i1, page_numbers_i1 = get_response_from_query(\n",
    "        db, query, temperature, k\n",
    "    )\n",
    "\n",
    "    # Stage 1\n",
    "    response1, confidence_score1 = perform_stage_1(llm, query, docs_page_content_i1)\n",
    "\n",
    "    if int(confidence_score1) >= CONFIDENCE_THRESHOLD:  \n",
    "        return response1, page_numbers_i1\n",
    "\n",
    "    # Stage 2\n",
    "    response2, confidence_score2 = perform_stage_2(llm, docs_page_content_i1, response1)\n",
    "\n",
    "    if int(confidence_score2) >= CONFIDENCE_THRESHOLD: \n",
    "        return response2, page_numbers_i1\n",
    "\n",
    "    # Stage 3\n",
    "    final_response = perform_stage_3(llm, docs_page_content_i1, response2)\n",
    "    print(final_response)\n",
    "\n",
    "    return final_response, page_numbers_i1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPT-3.5-Turbo with Basic Tree of Thought Reasoning**\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"https://github.com/ayyubibrahimi/llm-criminal-justice-research/assets/57520778/cb6c91ea-13b1-470b-a352-3c023b51c2e0\" alt=\"Figure 5: Average Cumulative F1 Beta Score for Police Reports and Court Transcripts - GPT-3.5-Turbo with Basic Tree of Thought Reasoning\" width=\"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative Analysis of F1 Beta Scores: Complex vs. Basic Tree of Thought Reasoning\n",
    "\n",
    "In evaluating the effectiveness of the two methodologies, the F1 Beta scores provide a quantitative measure of their performance. These scores, derived from tests on the GPT-3.5-Turbo model using both the complex and basic (simplified) Tree of Thought reasoning approaches, offer insights into their respective efficiencies in handling legal documents, specifically reports and transcripts.\n",
    "\n",
    "GPT-3.5-Turbo with Complex Tree of Thought Reasoning\n",
    "Under the complex Tree of Thought methodology:\n",
    "\n",
    "- Reports: Achieved an F1 Beta score of 0.64237.\n",
    "- Transcripts: Attained an F1 Beta score of 0.68455.\n",
    "\n",
    "These scores indicate the model's proficiency in processing and analyzing legal documents. The complex approach, with its multi-faceted and iterative strategy, demonstrates a commendable level of accuracy and thoroughness in information extraction.\n",
    "\n",
    "GPT-3.5-Turbo with Basic Tree of Thought Reasoning\n",
    "In contrast, the simplified or basic Tree of Thought reasoning approach yielded the following scores:\n",
    "\n",
    "- Reports: Recorded an F1 Beta score of 0.66325.\n",
    "- Transcripts: Reached an F1 Beta score of 0.69702.\n",
    "\n",
    "Interestingly, the basic approach, despite its reduced complexity and fewer API calls, outperforms the complex methodology in both categories. These scores are indicative of the efficacy of the streamlined process in accurately extracting and analyzing data from the given documents.\n",
    "\n",
    "Implications and Insights\n",
    "The comparative analysis of these F1 Beta scores offers several key insights:\n",
    "\n",
    "Efficiency vs. Complexity: The basic Tree of Thought approach demonstrates that a less complex methodology can achieve, and even surpass, the performance levels of a more intricate system. This finding challenges the notion that increased complexity always leads to better outcomes in AI-driven tasks.\n",
    "\n",
    "Resource Optimization: The higher F1 Beta scores achieved with fewer API calls in the basic approach underscore the importance of resource optimization. This aspect is particularly crucial in contexts where computational efficiency is as important as accuracy.\n",
    "\n",
    "Adaptability and Focus: The basic approach's adaptability within its simplified framework, focusing on iterative document review, proves effective in precisely identifying and extracting relevant information.\n",
    "\n",
    "In conclusion, the F1 Beta scores provide a compelling argument for the adoption of the basic Tree of Thought reasoning approach in scenarios where computational efficiency and accuracy are both priorities. This method’s ability to deliver high-quality results with a simpler, more resource-conscious framework makes it a valuable model for legal document analysis and similar AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Adjusted Parameters in Basic Tree of Thought Reasoning\n",
    "\n",
    "Building upon the initial findings from the basic Tree of Thought reasoning approach, we further refined the model by adjusting certain parameters. This modification aimed to maximize the token context window, providing the model with more comprehensive data for analysis. The revised parameters included maintaining the chunk size at 500 and the chunk overlap at 250, but increasing the value of k to 55. The F1 Beta scores obtained with these new parameters offer insight into the impact of these adjustments.\n",
    "\n",
    "The performance of the basic Tree of Thought model with the new parameters is as follows:\n",
    "\n",
    "- Reports: The F1 Beta score achieved for reports was 0.63817. This score, while slightly lower than the original basic model's score of 0.66325, still remains competitive, especially considering the increased complexity of the task due to the larger context window.\n",
    "\n",
    "- Transcripts: For transcripts, the model significantly outperformed its earlier iteration, achieving an F1 Beta score of 0.74530, compared to the original score of 0.69702. This notable improvement underscores the model's enhanced ability to handle more extensive data segments effectively.\n",
    "\n",
    "Analysis and Implications\n",
    "The results from these parameter adjustments lead to several important conclusions:\n",
    "\n",
    "Optimal Parameter Tuning: The increase in the F1 Beta score for transcripts indicates that the model benefits from a larger context window, allowing for a more detailed and comprehensive analysis of the documents.\n",
    "\n",
    "Trade-offs Between Parameters and Performance: The slight decrease in the score for reports suggests that there might be a trade-off between the size of the data chunk and the model's ability to maintain accuracy. This finding highlights the importance of fine-tuning model parameters to balance the breadth of context with the precision of analysis.\n",
    "\n",
    "Enhanced Performance in Complex Analysis: The significant improvement in transcript analysis demonstrates that the model, with adjusted parameters, is particularly adept at handling more complex and lengthy documents. This ability is crucial in legal and technical domains where documents are often extensive and intricate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPT-3.5-Turbo with Basic Tree of Thought Reasoning - New Parameters**\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"https://github.com/ayyubibrahimi/llm-criminal-justice-research/assets/57520778/0d33ba12-a2dc-4d8f-9b24-007c659a5540\" alt=\"Figure 5: Average Cumulative F1 Beta Score for Police Reports and Court Transcripts - GPT-3.5-Turbo with Basic Tree of Thought Reasoning and New Parameters\" width=\"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bar chart colors: plt.bar(categories, average_scores, color=['#CD5C5C', '#4682B4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand on\n",
    "\n",
    "- GPT-4-Turbo\n",
    "- GPT-3-Turbo-1106"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"appendix\"></a>\n",
    "# Appendix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gpt-4-evaluation\"></a>\n",
    "# Testing with GPT-4 (Original Parameters of 500, 250, and 20)\n",
    "\n",
    "GPT-4 Turbo should be mentioned in the next steps/future research section. The GPT-4-Turbo model with expanded parameters achieves near perfection. That said, it's still much more expensive. Therefore, all tests have been run with GPT-3\n",
    "\n",
    "- GPT-4 applied to Police Reports\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"https://github.com/ayyubibrahimi/llm-criminal-justice-research/assets/57520778/9873d28c-4780-4ae3-ab06-388ec662d8a2\" alt=\"Figure 5: Average Cumulative F1 Beta Score for 1 Query vs. 6 Queries (Police Reports) - Model: GPT-4\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "\n",
    "- GPT-4 applied to Court Transcripts\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"https://github.com/ayyubibrahimi/llm-criminal-justice-research/assets/57520778/ece48ac1-aa7d-412f-9819-44ce381dd30c\" alt=\"Figure 6: Average Cumulative F1 Beta Score for 1 Query vs. 6 Queries (Court Transcripts) - Model: GPT-4\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "\n",
    "- GPT-4-Turbo Applied to Police Reports\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"https://github.com/ayyubibrahimi/llm-criminal-justice-research/assets/57520778/25fc4b30-6ce2-4af2-90c3-28fefac1ab5e\" alt=\"Figure 6: Average Cumulative F1 Beta Score for 1 Query vs. 6 Queries (Police Reports) - Model: GPT-4-Turbo\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "- GPT-4-Turbo Applied to Court Transcripts\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"https://github.com/ayyubibrahimi/llm-criminal-justice-research/assets/57520778/54adbd92-b886-4dc3-8031-6cd9b4534e39\" alt=\"Figure 6: Average Cumulative F1 Beta Score for 1 Query vs. 6 Queries (Court Transcripts) - Model: GPT-4-Turbo\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "# Testing with GPT-4 (New Parameters of 20000, 6000, and 50)\n",
    "\n",
    "\n",
    "- GPT-4-Turbo with New Parameters Applied to Police Reports\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"https://github.com/ayyubibrahimi/llm-criminal-justice-research/assets/57520778/f0c4ea81-48e0-4afa-b43b-092304c55c32\" alt=\"Figure 6: Average Cumulative F1 Beta Score for 1 Query vs. 6 Queries (Police Reports) - Model: GPT-4-Turbo with New Parameters\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "- GPT-4-Turbo with New Parameters Applied to Court Transcripts\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"https://github.com/ayyubibrahimi/llm-criminal-justice-research/assets/57520778/36a85c71-592a-4085-95af-d5d258a6916d\" alt=\"Figure 6: Average Cumulative F1 Beta Score for 1 Query vs. 6 Queries (Court Transcripts) - Model: GPT-4-Turbo with New Parameters\" width=\"600\">\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <a id=\"acknowledgements\"></a>\n",
    "# Acknowledgements\n",
    "\n",
    "- IPNO team: Huy Dao\n",
    "- PDW team: Rajiv Sinclair, Khoi Pham\n",
    "- HRDAG: Dr. Megan Price\n",
    "- and OpenAI, for their generous donation of research credits, without which we could not have run all of these experiments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
